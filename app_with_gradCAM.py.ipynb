{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb913d2b-c90d-420c-85cc-9ffbecebebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bffb2fea-88f2-414a-940d-1377e603b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAM, EigenCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "import random\n",
    "import config\n",
    "import cv2\n",
    "from utils import non_max_suppression, cells_to_bboxes, get_bboxes, YoloCAM\n",
    "from PIL import Image\n",
    "\n",
    "# from models.resnet import ResNet18\n",
    "from fixed_model import s13Model, ScalePrediction, CNNBlock, ResidualBlock\n",
    "\n",
    "import gradio as gr\n",
    "model = s13Model(num_classes=config.NUM_CLASSES)\n",
    "\n",
    "# # new_model = model.load_from_checkpoint('s10Model.ckpt')\n",
    "model.load_state_dict(torch.load(\"/home/sn/object_detection_demo/s13Model.pth\", map_location=torch.device('cpu')), strict=False)\n",
    "model.eval()\n",
    "classes = [\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\"\n",
    "]\n",
    "\n",
    "# This will help us create a different color for each class\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "scaled_anchors = (torch.tensor(config.ANCHORS)\n",
    "    * torch.tensor(config.S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97da96e7-9f5c-4c10-8d62-50a33b56e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes):\n",
    "# Create a Rectangle patch\n",
    "    \n",
    "    image = np.array(image)\n",
    "    height, width, _ = image.shape\n",
    "    for box in boxes:\n",
    "        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n",
    "        class_pred = int(box[0])\n",
    "        box = box[2:]\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        lower_right_x = box[0] + box[2] / 2\n",
    "        lower_right_y = box[1] + box[3] / 2\n",
    "        color = COLORS[class_pred]\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(upper_left_x*width), int(upper_left_y*height)),\n",
    "            (int(lower_right_x*width), int(lower_right_y*height)),\n",
    "            color, 1\n",
    "        )\n",
    "        cv2.putText(image, f\"{classes[int(class_pred)]} : {box[1]:.2f}\", (int(upper_left_x*width), int(upper_left_y*height - 5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "def inference(input_img, transparency = 0.5): #, target_layer_number = -1):\n",
    "    org_img = input_img\n",
    "    input_img_aug = config.test_transforms(image=input_img, bboxes=[])\n",
    "    input_img = input_img_aug[\"image\"]\n",
    "    input_img = input_img\n",
    "    input_img = input_img.unsqueeze(0)\n",
    "    outputs = model(input_img)\n",
    "    obj_bboxes = get_bboxes(out=outputs, anchors=scaled_anchors)\n",
    "    obj_detected = draw_boxes(org_img, obj_bboxes)\n",
    "\n",
    "    grayscale_cam = cam.forward(input_tensor=input_img, scaled_anchors=scaled_anchors, targets=None)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    visualization = show_cam_on_image(org_img/255, grayscale_cam, use_rgb=True, image_weight=transparency)\n",
    "\n",
    "    return obj_detected/255, visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3985458-6bb6-42f7-ad6e-115944a573f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up gradcam instance\n",
    "target_layer_number = -2\n",
    "target_layers = [model.layers[target_layer_number]]\n",
    "cam = YoloCAM(model=model, target_layers=target_layers, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89f0016e-6520-4b39-97e2-de59cabc347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = \"PASCAL VOC 2007 Dataset trained on Custom Model with GradCAM\"\n",
    "description = \"A simple Gradio interface for Object Detection using Custom model, and get GradCAM\"\n",
    "examples = [ #[\"example_imgs/cat.jpeg\", 0.5], \n",
    "            [\"example_imgs/dog.jpeg\", 0.5],\n",
    "            ['example_imgs/000030.jpg', 0.6],\n",
    "            ['example_imgs/000050.jpg', 0.5],\n",
    "            ['example_imgs/dogs.jpeg', 0.6],\n",
    "            ['example_imgs/train.jpeg', 0.6],\n",
    "            ['example_imgs/bird1.jpeg', 0.7],\n",
    "            ['example_imgs/cars1.jpeg', 0.5],\n",
    "            ['example_imgs/horse1.jpeg', 0.6],\n",
    "            # ['example_imgs/train2.jpeg'],\n",
    "            # ['example_imgs/bird2.jpeg'],\n",
    "            # ['example_imgs/cars2.jpeg'],\n",
    "            # ['example_imgs/horse2.jpeg'],\n",
    "\n",
    "           ]\n",
    "demo = gr.Interface(\n",
    "    inference, \n",
    "    inputs = [gr.Image(shape=(416, 416), label=\"Input Image\"), \n",
    "              gr.Slider(0, 1, value = 0.5, label=\"Opacity of GradCAM\"),\n",
    "             ],\n",
    "    outputs = gr.Gallery(rows=2, columns=1, min_width=416),\n",
    "    title = title,\n",
    "    description = description,\n",
    "    examples = examples,\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255aa4d-ef0c-4d29-bcb5-a7649c816d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai2022)",
   "language": "python",
   "name": "fastai2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
